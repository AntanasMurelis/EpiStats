{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a pipeline for labels preprocessing, statistics collection and cells meshing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.measure import regionprops, regionprops_table\n",
    "from typing import Dict, List, Optional, Union, Tuple, Callable\n",
    "import concurrent\n",
    "import trimesh as tm\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('src')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import misc \n",
    "import SegmentationStatisticsCollector \n",
    "import LabelPreprocessing\n",
    "import GenMeshes\n",
    "from StatsCollection import *\n",
    "from ExtendedTrimesh import ExtendedTrimesh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess curated segmentation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load curated segmentation\n",
    "PATH_TO_LABELED_IMG = '/nas/groups/iber/Users/Federico_Carrara/create_meshes/data/curated_labels/'\n",
    "FILE_NAME = 'MBC19_S5_St1_Crop_GFP_clean_bottom.tif'\n",
    "\n",
    "labeled_img = misc.load_labeled_img(os.path.join(PATH_TO_LABELED_IMG, FILE_NAME))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create output folder\n",
    "smoothing_iters = 10\n",
    "erosion_iters = 6\n",
    "dilation_iters = 8\n",
    "\n",
    "PATH_TO_OUTPUT = './tests/output'\n",
    "output_dir = misc.create_output_directory(\n",
    "    output_folder=PATH_TO_OUTPUT, \n",
    "    input_img_path=os.path.join(PATH_TO_LABELED_IMG, FILE_NAME),\n",
    "    smoothing_iterations=smoothing_iters, \n",
    "    erosion_iterations=erosion_iters, \n",
    "    dilation_iterations=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess labels\n",
    "process = True\n",
    "\n",
    "if process:\n",
    "    preprocessed_labeled_img = LabelPreprocessing.process_labels(\n",
    "        labeled_img=labeled_img, \n",
    "        erosion_iterations=erosion_iters,\n",
    "        dilation_iterations=dilation_iters,\n",
    "        output_directory=output_dir,\n",
    "        overwrite=False\n",
    "    )\n",
    "else:\n",
    "    preprocessed_labeled_img = labeled_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to filter cells which are not good for computing morphological statistics.\n",
    "\n",
    "We want to get the following:\n",
    "- A list of indexes of cut cells, i.e., cells which are touching the background of the image --> VoxelProcessing.remove_labels_touching_edges\n",
    "- A list of indexes of cells which touches the background --> VoxelProcessing.remove_labels_touching_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_labeled_img = preprocessed_labeled_img[:100, :100, :100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter cells for different statistics\n",
    "cut_cells_idxs = LabelPreprocessing.get_labels_touching_edges(preprocessed_labeled_img, output_dir)\n",
    "touching_background_idxs, background_touch_counts = LabelPreprocessing.get_labels_touching_background(preprocessed_labeled_img, output_dir, threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.intersect1d(cut_cells_idxs, touching_background_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_filtered_labeled_img = LabelPreprocessing.filter_labels(preprocessed_labeled_img, cut_cells_idxs)\n",
    "bg_filtered_labeled_img = LabelPreprocessing.filter_labels(preprocessed_labeled_img, touching_background_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(np.unique(cut_filtered_labeled_img)), len(np.unique(bg_filtered_labeled_img)))\n",
    "print(len(np.unique(preprocessed_labeled_img)))\n",
    "print(len(cut_cells_idxs), len(touching_background_idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_img = preprocessed_labeled_img.copy()\n",
    "binary_mask = try_img == touching_background_idxs[0]\n",
    "try_img[binary_mask] = 0\n",
    "print(len(np.unique(try_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_labels(preprocessed_labeled_img)\n",
    "viewer.add_labels(cut_filtered_labeled_img)\n",
    "viewer.add_labels(bg_filtered_labeled_img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mesh Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshes = GenMeshes.convert_labels_to_meshes(\n",
    "    preprocessed_labeled_img,\n",
    "    [0.1625, 0.1625, 0.25],\n",
    "    cut_cells_idxs,\n",
    "    10,\n",
    "    output_dir,\n",
    "    False,\n",
    "    10,\n",
    "    'stl'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistics collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_areas = compute_cell_surface_areas(meshes, cut_cells_idxs)\n",
    "cell_volumes = compute_cell_volumes(meshes, cut_cells_idxs)\n",
    "cell_axes, cell_elong = compute_cell_principal_axis_and_elongation(meshes, cut_cells_idxs)\n",
    "# cell_neighbors = compute_cell_neighbors(preprocessed_labeled_img, cut_cells_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_contact_area = compute_cell_contact_area(meshes, cell_neighbors, 0.1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # temporary: store statistics long to compute in json\n",
    "\n",
    "# def save_dictionary(dictionary, filename):\n",
    "#     with open(filename, 'wb') as file:\n",
    "#         pickle.dump(dictionary, file)\n",
    "\n",
    "# # save neighbors\n",
    "# if not os.path.exists('tests/output_s_10_e_6_d_8/temp_stats/'):\n",
    "#     os.makedirs('tests/output_s_10_e_6_d_8/temp_stats/')\n",
    "# save_dictionary(cell_neighbors, 'tests/output_s_10_e_6_d_8/temp_stats/cell_neighbors.pickle')\n",
    "# save_dictionary(cell_contact_area, 'tests/output_s_10_e_6_d_8/temp_stats/cell_contact_area.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tests/output_s_10_e_6_d_8/temp_stats/cell_neighbors.pickle', 'rb') as f:\n",
    "    cell_neighbors = pickle.load(f)\n",
    "\n",
    "with open('tests/output_s_10_e_6_d_8/temp_stats/cell_contact_area.pickle', 'rb') as f:\n",
    "    cell_contact_area = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsCollector:\n",
    "    def __init__(\n",
    "            self,\n",
    "            meshes: Dict[int, tm.base.Trimesh],\n",
    "            labels: np.ndarray[int],\n",
    "            # original_ids: List[int],\n",
    "            features: List[str],\n",
    "            output_directory: str,\n",
    "            path_to_img: str,\n",
    "            tissue: str,\n",
    "            num_workers: int\n",
    "        ) -> None:\n",
    "\n",
    "        #internal attributes\n",
    "        self._features_to_functions = StatsCollector._feat_to_func_dict()\n",
    "        self._tissues_to_types = StatsCollector._tissue_to_type_dict()\n",
    "        \n",
    "        #public attributes\n",
    "        self.meshes = meshes\n",
    "        self.labels = labels\n",
    "        self.ids = list(self.meshes.keys())\n",
    "        # self.original_ids = original_ids\n",
    "        self.features = features \n",
    "        self.functions = [\n",
    "            self._features_to_functions[feature] \n",
    "            for feature in self.features\n",
    "        ]\n",
    "        self.tissue = tissue\n",
    "        self.tissue_type = self._tissues_to_types[tissue]\n",
    "        self.output_dir = output_directory\n",
    "        self.df_output_dir = os.path.join(self.output_dir, 'cell_stats')\n",
    "        self.path_to_img = path_to_img\n",
    "        file_name = os.path.basename(self.path_to_img)\n",
    "        self.file_ext = os.path.splitext(file_name)[1]\n",
    "        self.file_name = file_name.replace(self.file_ext, '')\n",
    "        self.num_workers = num_workers\n",
    "        self.cache_dir = os.path.join(self.df_output_dir, 'cached_stats')\n",
    "        if not os.path.exists(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "\n",
    "        #apply filtering to get labels to be excluded from computation\n",
    "        self.excluded_idxs = self.filter_cells()\n",
    "\n",
    "        #initialize and save the dataframe to store statistics\n",
    "        self.df = self._init_dataframe()\n",
    "        #save the newly created data structure\n",
    "        self._save_dataframe()\n",
    "\n",
    "    @staticmethod\n",
    "    def _feat_to_func_dict() -> Dict[str, Callable]:\n",
    "        features = ['area', 'volume', 'elongation_and_axes',\n",
    "                    'neighbors', 'contact_area']\n",
    "        functions = [\n",
    "            compute_cell_surface_areas,\n",
    "            compute_cell_volumes,\n",
    "            compute_cell_principal_axis_and_elongation,\n",
    "            compute_cell_neighbors,\n",
    "            compute_cell_contact_area\n",
    "        ]\n",
    "\n",
    "        return dict(zip(features, functions))\n",
    "\n",
    "    @staticmethod\n",
    "    def _tissue_to_type_dict() -> Dict[str, str]:\n",
    "        tissues = ['bladder', 'intestine_villus', 'lung_bronchiole', 'esophagus']\n",
    "        tissue_types = ['stratified_transitional', 'simple_columnar', 'simple_cuboidal', 'stratified_squamous']\n",
    "\n",
    "        return dict(zip(tissues, tissue_types))\n",
    "    \n",
    "    \n",
    "    def filter_cells(self) -> List[int]:\n",
    "        if 'simple' in self.tissue_type:\n",
    "            idxs_to_filter = LabelPreprocessing.get_labels_touching_edges(\n",
    "                self.labels, self.output_dir\n",
    "            )\n",
    "        elif 'stratified' in self.tissue_type:\n",
    "            raise NotImplementedError()\n",
    "            # idxs_to_filter = LabelPreprocessing.get_labels_touching_edges(\n",
    "            #     self.labels, self.output_dir\n",
    "            # )\n",
    "        \n",
    "        return idxs_to_filter\n",
    "        \n",
    "\n",
    "    def _save_dataframe(\n",
    "            self,\n",
    "            overwrite: bool = True\n",
    "    ) -> None:\n",
    "        \n",
    "        if not os.path.exists(self.df_output_dir):\n",
    "            os.makedirs(self.df_output_dir)\n",
    "\n",
    "        path_to_file = os.path.join(self.df_output_dir, self.file_name)\n",
    "        if (not os.path.isfile(path_to_file)) or overwrite:\n",
    "            self.df.to_csv(path_to_file)\n",
    "\n",
    "    \n",
    "    def _init_dataframe(self) -> pd.DataFrame:\n",
    "        #initialize the data structure\n",
    "        df = pd.DataFrame(\n",
    "            data={\n",
    "                'cell_ID': self.ids,\n",
    "                'tissue': self.tissue,\n",
    "                'file_name': self.path_to_img\n",
    "                # 'original_cell_ID': self.original_ids\n",
    "            }\n",
    "        )\n",
    "        df['mesh_dir'] = [\n",
    "            os.path.join(self.output_dir, 'cell_meshes', f'cell_{id}.stl')\n",
    "            for id in self.ids\n",
    "        ]\n",
    "        df['exclude_cell'] = [id in self.excluded_idxs for id in self.ids]\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _unpack_feature_dict(\n",
    "            feature_dict: Dict[int, any]\n",
    "        ) -> pd.Series:\n",
    "        '''\n",
    "        Unpack the dictionary associated to each feature in a pd.Series.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            feature_dict: (Dict[int, any])\n",
    "                A dict whose keys are cell ids and values are the associated statistics value.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "            feature_unpacked: (pd.Series[any])\n",
    "                A pd.Series of the statistics values.\n",
    "        '''\n",
    "        feature_unpacked = pd.Series(list(feature_dict.values()))\n",
    "        return feature_unpacked\n",
    "    \n",
    "    \n",
    "    def _add_to_dataframe(\n",
    "        self,\n",
    "        feature_dict: Dict[int, any],\n",
    "        feature_name: str,\n",
    "    ) -> None:\n",
    "\n",
    "        #unpack the dictionary\n",
    "        feature_data = StatsCollector._unpack_feature_dict(feature_dict)\n",
    "\n",
    "        #add column to df\n",
    "        self.df[feature_name] = feature_data \n",
    "\n",
    "    def _to_cache(\n",
    "            self,\n",
    "            feature_dict: Dict[int, any], \n",
    "            feature_name: str\n",
    "    ) -> None:\n",
    "\n",
    "        save_name = f'cell_{feature_name}.pickle'\n",
    "        with open(os.path.join(self.cache_dir, save_name), 'wb') as file:\n",
    "            pickle.dump(feature_dict, file)\n",
    "\n",
    "    \n",
    "    def _from_cache(\n",
    "            self,\n",
    "            feature_name: str\n",
    "    ) -> Dict[int, any]:\n",
    "\n",
    "        assert os.path.exists(self.cache_dir), 'Cannot load from cache as it is empty.'\n",
    "\n",
    "        save_name = f'cached_stats/cell_{feature_name}.pickle'\n",
    "        with open(os.path.join(self.df_output_dir, save_name), 'rb') as file:\n",
    "            feature_dict = pickle.load(file)\n",
    "    \n",
    "        return feature_dict\n",
    "    \n",
    "\n",
    "    def _get_args(\n",
    "            self,\n",
    "            feature_name: str,\n",
    "    ) -> List[any]:\n",
    "        if feature_name == 'neighbors':\n",
    "            args = (\n",
    "                self.labels,\n",
    "                self.excluded_idxs\n",
    "            )\n",
    "        elif feature_name == 'contact_area':\n",
    "            try:\n",
    "                neighbors_dict = self._from_cache('neighbors')\n",
    "            except Exception as e:\n",
    "                raise OSError('Neighbors dictionary not found in the cache.') from e\n",
    "            args = (\n",
    "                self.meshes,\n",
    "                neighbors_dict,\n",
    "                self.num_workers\n",
    "            )\n",
    "        else:\n",
    "            args = (\n",
    "                self.meshes,\n",
    "                self.excluded_idxs\n",
    "            )\n",
    "\n",
    "        return args\n",
    "\n",
    "\n",
    "    def _process_df(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        #count neighbors\n",
    "        self.df['num_neighbors'] = self.df['neighbors'].apply(lambda x: len(x))\n",
    "\n",
    "        #split elongation and principal axes\n",
    "        self.df['elongation'] = self.df['elongation_and_axes'].apply(lambda x: x[0])\n",
    "        self.df['principal_axes'] = self.df['elongation_and_axes'].apply(lambda x: x[1])\n",
    "        self.df.drop(columns=['elongation_and_axes'], inplace=True)\n",
    "\n",
    "        #split and extract statistics from contact area\n",
    "        self.df['contact_area_fraction'] = self.df['contact_area'].apply(lambda x: x[0])\n",
    "        self.df['contact_area_distribution'] = self.df['contact_area'].apply(lambda x: x[1])\n",
    "        self.df['mean_contact_area'] = self.df['contact_area_distribution'].apply(lambda x: np.mean(x))\n",
    "        self.df['total_contact_area'] = self.df['contact_area_distribution'].apply(lambda x: np.sum(x))\n",
    "        self.df.drop(columns='contact_area', inplace=True)\n",
    "\n",
    "\n",
    "    def collect_statistics(\n",
    "            self,\n",
    "            load_from_cache: Optional[bool] = False\n",
    "    ) -> None:\n",
    "        \n",
    "        for func, feat in zip(self.functions, self.features):\n",
    "            if load_from_cache:\n",
    "                print(f'Loading cached cell {feat} ...')\n",
    "                feat_dict = self._from_cache(feat)\n",
    "            else:\n",
    "                args = self._get_args(feat)\n",
    "                feat_dict = func(*args)\n",
    "\n",
    "                #cache the dict\n",
    "                self._to_cache(feat_dict, feat)\n",
    "            \n",
    "            #add the new stat to the dataframe\n",
    "            self._add_to_dataframe(feat_dict, feat)\n",
    "\n",
    "            #save the dataframe\n",
    "            self._save_dataframe(overwrite=True)\n",
    "\n",
    "        #postprocess dataframe\n",
    "        self._process_df()\n",
    "        self._save_dataframe(overwrite=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_collector = StatsCollector(\n",
    "    meshes=meshes,\n",
    "    labels=preprocessed_labeled_img,\n",
    "    # original_ids=list(meshes.keys()),\n",
    "    features=['area', 'volume', 'elongation_and_axes', 'neighbors', 'contact_area'],\n",
    "    output_directory='tests/output_s_10_e_6_d_8',\n",
    "    path_to_img='tests/output_s_10_e_6_d_8/processed_labels.npy',\n",
    "    tissue='lung_bronchiole',\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_collector.collect_statistics(load_from_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_collector.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_collector.df[\"isoper_ratio\"] = stats_collector.df.apply(lambda x: x['area']**3/x['volume']**2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_collector.df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3d-seg-fcarrara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
